#!/bin/bash
## ============== This is the configuration proper to my cluster (CentraleSup√©lec's DGX) ==============
## Here it use the prod20 partition but you can change it to prod10, prod40 or prod80 by commenting/uncommenting the corresponding lines

#SBATCH --job-name=research_template
#SBATCH --output=out.txt
#SBATCH --error=out.txt

## For partition: either prod10, prod 20, prod 40 or prod80
#SBATCH --partition=prod10

## For gres: either 1g.10gb:[1:10] for prod10, 2g.20gb:[1:4] for prod20, 3g.40gb:1 for prod40 or A100.80gb for prod80.

##SBATCH --partition=prod10
##SBATCH --gres=gpu:1g.10gb:1
##SBATCH --cpus-per-task=4

#SBATCH --partition=prod20
#SBATCH --gres=gpu:2g.20gb:1
#SBATCH --cpus-per-task=4

##SBATCH --partition=prod40
##SBATCH --gres=gpu:3g.40gb:1
##SBATCH --cpus-per-task=4

##SBATCH --partition=prod80
##SBATCH --gres=gpu:A100.80gb:1
##SBATCH --ntasks-per-node=1
##SBATCH --cpus-per-task=8
##SBATCH --mem-per-cpu=10G
##SBATCH --nodes=1

## For ntasks and cpus: total requested cpus (ntasks * cpus-per-task) must be in [1: 4 * nMIG] with nMIG = nb_1g.10gb | 2 * nb_2g.20gb | 4 * nb_3g.40gb | 8 * nb_A100.80gb
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4

## Walltime limit
#SBATCH --time=24:00:00



## ============== Run your job here ==============

## Setup
source ~/mva_geom/mva_geom_24/venv/bin/activate # my own base Python venv, 

source ../venv/bin/activate
source venv/bin/activate # if you setup yourself a venv in your repo, it should pick it 

cd ..



python train.py 
